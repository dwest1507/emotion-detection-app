{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX Model Export and Validation\n",
    "\n",
    "This notebook exports the trained emotion classification model to ONNX format for production deployment.\n",
    "\n",
    "## Objectives\n",
    "1. Export PyTorch model to ONNX format\n",
    "2. Validate ONNX model with onnxruntime\n",
    "3. Compare PyTorch vs ONNX outputs\n",
    "4. Measure inference time\n",
    "5. Verify model size requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model configuration\n",
    "NUM_CLASSES = 7\n",
    "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(EMOTIONS)}\n",
    "IDX_TO_EMOTION = {idx: emotion for emotion, idx in EMOTION_TO_IDX.items()}\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Model Architecture and Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model loaded successfully from ../models/final_model.pth\n",
      "  Model architecture: EfficientNet-B0\n",
      "  Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_classes=7, freeze_backbone=False):\n",
    "    \"\"\"\n",
    "    Create EfficientNet-B0 model with transfer learning.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of output classes (7 for FER-2013)\n",
    "        freeze_backbone: If True, freeze feature extraction layers\n",
    "    \n",
    "    Returns:\n",
    "        model: PyTorch model\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNet-B0\n",
    "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
    "    \n",
    "    # Freeze backbone layers (not needed for inference)\n",
    "    if freeze_backbone:\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Replace classifier head for our number of classes\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(p=0.3),\n",
    "        nn.Linear(in_features, num_classes)\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = create_model(num_classes=NUM_CLASSES, freeze_backbone=False)\n",
    "model_path = Path('../models/final_model.pth')\n",
    "\n",
    "if model_path.exists():\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    print(f\"\u2713 Model loaded successfully from {model_path}\")\n",
    "    print(f\"  Model architecture: EfficientNet-B0\")\n",
    "    print(f\"  Number of classes: {NUM_CLASSES}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Export to ONNX Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model to ONNX format...\n",
      "  Model device: CPU\n",
      "  Input shape: torch.Size([1, 3, 224, 224])\n",
      "  Output path: ../models/emotion_classifier.onnx\n",
      "[torch.onnx] Obtain model graph for `EfficientNet([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `EfficientNet([...]` with `torch.export.export(..., strict=False)`... \u2705\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... \u2705\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... \u2705\n",
      "Applied 98 of general pattern rewrite rules.\n",
      "\u2713 Model exported successfully to ../models/emotion_classifier.onnx\n",
      "  Model size: 0.50 MB\n",
      "  \u2713 Model size is under 30MB requirement\n"
     ]
    }
   ],
   "source": [
    "# Move model to CPU for ONNX export (ONNX export works better on CPU)\n",
    "model_cpu = model.cpu()\n",
    "model_cpu.eval()\n",
    "\n",
    "# Create dummy input on CPU (batch_size=1, channels=3, height=224, width=224)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# ONNX export path\n",
    "onnx_path = Path('../models/emotion_classifier.onnx')\n",
    "onnx_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "print(f\"  Model device: CPU\")\n",
    "print(f\"  Input shape: {dummy_input.shape}\")\n",
    "print(f\"  Output path: {onnx_path}\")\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(\n",
    "    model_cpu,                      # Model to export (on CPU)\n",
    "    dummy_input,                    # Dummy input (on CPU)\n",
    "    str(onnx_path),                 # Output file path\n",
    "    export_params=True,              # Store trained parameter weights\n",
    "    opset_version=18,                # ONNX opset version (18 is default for PyTorch 2.9+)\n",
    "    do_constant_folding=True,       # Execute constant folding optimization\n",
    "    input_names=['input'],          # Input tensor name\n",
    "    output_names=['output'],        # Output tensor name\n",
    "    dynamic_axes={\n",
    "        'input': {0: 'batch_size'},  # Variable batch size\n",
    "        'output': {0: 'batch_size'}  # Variable batch size\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"\u2713 Model exported successfully to {onnx_path}\")\n",
    "\n",
    "# Check file size\n",
    "file_size_mb = onnx_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"  Model size: {file_size_mb:.2f} MB\")\n",
    "\n",
    "if file_size_mb < 30:\n",
    "    print(f\"  \u2713 Model size is under 30MB requirement\")\n",
    "else:\n",
    "    print(f\"  \u26a0\ufe0f  Model size exceeds 30MB requirement\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate ONNX Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 ONNX model validation passed!\n",
      "\n",
      "ONNX Model Information:\n",
      "  IR Version: 10\n",
      "  Producer: pytorch 2.9.1+cu130\n",
      "  Opset Version: 18\n",
      "  Inputs: ['input']\n",
      "  Outputs: ['output']\n"
     ]
    }
   ],
   "source": [
    "# Load and validate ONNX model\n",
    "onnx_model = onnx.load(str(onnx_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"\u2713 ONNX model validation passed!\")\n",
    "\n",
    "# Print model information\n",
    "print(f\"\\nONNX Model Information:\")\n",
    "print(f\"  IR Version: {onnx_model.ir_version}\")\n",
    "print(f\"  Producer: {onnx_model.producer_name} {onnx_model.producer_version}\")\n",
    "print(f\"  Opset Version: {onnx_model.opset_import[0].version}\")\n",
    "print(f\"  Inputs: {[input.name for input in onnx_model.graph.input]}\")\n",
    "print(f\"  Outputs: {[output.name for output in onnx_model.graph.output]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load ONNX Runtime Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 ONNX Runtime session created\n",
      "  Input name: input\n",
      "  Output name: output\n",
      "  Input shape: ['s77', 3, 224, 224]\n",
      "  Output shape: [1, 7]\n",
      "  Providers: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "# Create ONNX Runtime session\n",
    "session = ort.InferenceSession(str(onnx_path), providers=['CPUExecutionProvider'])\n",
    "\n",
    "# Get input/output names\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "print(\"\u2713 ONNX Runtime session created\")\n",
    "print(f\"  Input name: {input_name}\")\n",
    "print(f\"  Output name: {output_name}\")\n",
    "print(f\"  Input shape: {session.get_inputs()[0].shape}\")\n",
    "print(f\"  Output shape: {session.get_outputs()[0].shape}\")\n",
    "print(f\"  Providers: {session.get_providers()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare PyTorch vs ONNX Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing PyTorch and ONNX outputs on 100 random inputs...\n",
      "Tolerance: 0.001\n",
      "Note: Small differences in logits are expected due to numerical precision differences\n",
      "For classification, what matters is whether the predicted classes match.\n",
      "\n",
      "  Sample 1: Small diff (pred \u2713) | Logit diff: 0.016541 | PyTorch: fear | ONNX: fear\n",
      "  Sample 2: Small diff (pred \u2713) | Logit diff: 0.190063 | PyTorch: angry | ONNX: angry\n",
      "  Sample 3: Small diff (pred \u2713) | Logit diff: 0.094482 | PyTorch: angry | ONNX: angry\n",
      "  Sample 4: Small diff (pred \u2713) | Logit diff: 0.001160 | PyTorch: fear | ONNX: fear\n",
      "  Sample 5: Small diff (pred \u2713) | Logit diff: 0.050377 | PyTorch: fear | ONNX: fear\n",
      "  Sample 6: Small diff (pred \u2713) | Logit diff: 0.170776 | PyTorch: fear | ONNX: fear\n",
      "  Sample 7: Small diff (pred \u2713) | Logit diff: 0.006500 | PyTorch: sad | ONNX: sad\n",
      "  Sample 8: Small diff (pred \u2713) | Logit diff: 0.035141 | PyTorch: fear | ONNX: fear\n",
      "  Sample 9: Small diff (pred \u2713) | Logit diff: 0.124695 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 10: Small diff (pred \u2713) | Logit diff: 0.002258 | PyTorch: fear | ONNX: fear\n",
      "  Sample 11: Small diff (pred \u2713) | Logit diff: 0.006271 | PyTorch: fear | ONNX: fear\n",
      "  Sample 12: Small diff (pred \u2713) | Logit diff: 0.001312 | PyTorch: sad | ONNX: sad\n",
      "  Sample 13: Small diff (pred \u2713) | Logit diff: 0.004755 | PyTorch: fear | ONNX: fear\n",
      "  Sample 14: Small diff (pred \u2713) | Logit diff: 0.002670 | PyTorch: fear | ONNX: fear\n",
      "  Sample 15: Small diff (pred \u2713) | Logit diff: 0.001354 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 16: Small diff (pred \u2713) | Logit diff: 0.011271 | PyTorch: angry | ONNX: angry\n",
      "  Sample 17: Small diff (pred \u2713) | Logit diff: 0.005249 | PyTorch: sad | ONNX: sad\n",
      "  Sample 18: Small diff (pred \u2713) | Logit diff: 0.009739 | PyTorch: fear | ONNX: fear\n",
      "  Sample 19: Small diff (pred \u2713) | Logit diff: 0.066925 | PyTorch: fear | ONNX: fear\n",
      "  Sample 20: Small diff (pred \u2713) | Logit diff: 0.009491 | PyTorch: fear | ONNX: fear\n",
      "  Sample 21: \u26a0\ufe0f  Large diff (pred \u2713) | Logit diff: 1.205078 | PyTorch: sad | ONNX: sad\n",
      "  Sample 22: Small diff (pred \u2713) | Logit diff: 0.145935 | PyTorch: sad | ONNX: sad\n",
      "  Sample 23: Small diff (pred \u2713) | Logit diff: 0.003052 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 24: Small diff (pred \u2713) | Logit diff: 0.002716 | PyTorch: fear | ONNX: fear\n",
      "  Sample 25: Small diff (pred \u2713) | Logit diff: 0.326355 | PyTorch: fear | ONNX: fear\n",
      "  Sample 26: Small diff (pred \u2713) | Logit diff: 0.001648 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 27: Small diff (pred \u2713) | Logit diff: 0.017487 | PyTorch: fear | ONNX: fear\n",
      "  Sample 28: Small diff (pred \u2713) | Logit diff: 0.010147 | PyTorch: fear | ONNX: fear\n",
      "  Sample 29: Small diff (pred \u2713) | Logit diff: 0.004295 | PyTorch: fear | ONNX: fear\n",
      "  Sample 30: Small diff (pred \u2713) | Logit diff: 0.016022 | PyTorch: sad | ONNX: sad\n",
      "  Sample 31: Small diff (pred \u2713) | Logit diff: 0.004673 | PyTorch: fear | ONNX: fear\n",
      "  Sample 32: Small diff (pred \u2713) | Logit diff: 0.002491 | PyTorch: fear | ONNX: fear\n",
      "  Sample 33: Small diff (pred \u2713) | Logit diff: 0.238914 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 34: Small diff (pred \u2713) | Logit diff: 0.716309 | PyTorch: fear | ONNX: fear\n",
      "  Sample 35: Small diff (pred \u2713) | Logit diff: 0.042084 | PyTorch: fear | ONNX: fear\n",
      "  Sample 36: Small diff (pred \u2713) | Logit diff: 0.001953 | PyTorch: sad | ONNX: sad\n",
      "  Sample 37: Small diff (pred \u2713) | Logit diff: 0.005737 | PyTorch: sad | ONNX: sad\n",
      "  Sample 38: Small diff (pred \u2713) | Logit diff: 0.914490 | PyTorch: fear | ONNX: fear\n",
      "  Sample 39: Small diff (pred \u2713) | Logit diff: 0.001770 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 40: Small diff (pred \u2713) | Logit diff: 0.005127 | PyTorch: sad | ONNX: sad\n",
      "  Sample 41: \u26a0\ufe0f  Large diff (pred \u2713) | Logit diff: 2.074066 | PyTorch: fear | ONNX: fear\n",
      "  Sample 42: Small diff (pred \u2713) | Logit diff: 0.059280 | PyTorch: fear | ONNX: fear\n",
      "  Sample 43: Small diff (pred \u2713) | Logit diff: 0.076400 | PyTorch: fear | ONNX: fear\n",
      "  Sample 44: Small diff (pred \u2713) | Logit diff: 0.001068 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 45: \u2713 Match | Logit diff: 0.000549 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 46: \u2713 Match | Logit diff: 0.000099 | PyTorch: fear | ONNX: fear\n",
      "  Sample 47: Small diff (pred \u2713) | Logit diff: 0.006622 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 48: Small diff (pred \u2713) | Logit diff: 0.003998 | PyTorch: fear | ONNX: fear\n",
      "  Sample 49: Small diff (pred \u2713) | Logit diff: 0.053894 | PyTorch: fear | ONNX: fear\n",
      "  Sample 50: \u2713 Match | Logit diff: 0.000813 | PyTorch: fear | ONNX: fear\n",
      "  Sample 51: Small diff (pred \u2713) | Logit diff: 0.018082 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 52: Small diff (pred \u2713) | Logit diff: 0.001617 | PyTorch: fear | ONNX: fear\n",
      "  Sample 53: \u2713 Match | Logit diff: 0.000732 | PyTorch: fear | ONNX: fear\n",
      "  Sample 54: Small diff (pred \u2713) | Logit diff: 0.009155 | PyTorch: sad | ONNX: sad\n",
      "  Sample 55: Small diff (pred \u2713) | Logit diff: 0.004044 | PyTorch: sad | ONNX: sad\n",
      "  Sample 56: Small diff (pred \u2713) | Logit diff: 0.062050 | PyTorch: fear | ONNX: fear\n",
      "  Sample 57: Small diff (pred \u2713) | Logit diff: 0.001526 | PyTorch: fear | ONNX: fear\n",
      "  Sample 58: Small diff (pred \u2713) | Logit diff: 0.006226 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 59: Small diff (pred \u2713) | Logit diff: 0.072106 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 60: Small diff (pred \u2713) | Logit diff: 0.004288 | PyTorch: fear | ONNX: fear\n",
      "  Sample 61: Small diff (pred \u2713) | Logit diff: 0.012817 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 62: Small diff (pred \u2713) | Logit diff: 0.002930 | PyTorch: fear | ONNX: fear\n",
      "  Sample 63: Small diff (pred \u2713) | Logit diff: 0.265976 | PyTorch: fear | ONNX: fear\n",
      "  Sample 64: Small diff (pred \u2713) | Logit diff: 0.001205 | PyTorch: fear | ONNX: fear\n",
      "  Sample 65: \u2713 Match | Logit diff: 0.000610 | PyTorch: angry | ONNX: angry\n",
      "  Sample 66: Small diff (pred \u2713) | Logit diff: 0.017273 | PyTorch: fear | ONNX: fear\n",
      "  Sample 67: Small diff (pred \u2713) | Logit diff: 0.014160 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 68: Small diff (pred \u2713) | Logit diff: 0.002136 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 69: \u2713 Match | Logit diff: 0.000259 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 70: \u26a0\ufe0f  Large diff (pred \u2713) | Logit diff: 5.956818 | PyTorch: fear | ONNX: fear\n",
      "  Sample 71: \u2713 Match | Logit diff: 0.000572 | PyTorch: fear | ONNX: fear\n",
      "  Sample 72: Small diff (pred \u2713) | Logit diff: 0.004562 | PyTorch: fear | ONNX: fear\n",
      "  Sample 73: Small diff (pred \u2713) | Logit diff: 0.018860 | PyTorch: happy | ONNX: happy\n",
      "  Sample 74: Small diff (pred \u2713) | Logit diff: 0.188110 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 75: Small diff (pred \u2713) | Logit diff: 0.021423 | PyTorch: fear | ONNX: fear\n",
      "  Sample 76: Small diff (pred \u2713) | Logit diff: 0.012844 | PyTorch: angry | ONNX: angry\n",
      "  Sample 77: Small diff (pred \u2713) | Logit diff: 0.003845 | PyTorch: fear | ONNX: fear\n",
      "  Sample 78: Small diff (pred \u2713) | Logit diff: 0.025803 | PyTorch: fear | ONNX: fear\n",
      "  Sample 79: Small diff (pred \u2713) | Logit diff: 0.012299 | PyTorch: fear | ONNX: fear\n",
      "  Sample 80: Small diff (pred \u2713) | Logit diff: 0.029068 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 81: Small diff (pred \u2713) | Logit diff: 0.264420 | PyTorch: happy | ONNX: happy\n",
      "  Sample 82: Small diff (pred \u2713) | Logit diff: 0.029892 | PyTorch: fear | ONNX: fear\n",
      "  Sample 83: Small diff (pred \u2713) | Logit diff: 0.115891 | PyTorch: fear | ONNX: fear\n",
      "  Sample 84: Small diff (pred \u2713) | Logit diff: 0.125122 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 85: Small diff (pred \u2713) | Logit diff: 0.001465 | PyTorch: fear | ONNX: fear\n",
      "  Sample 86: Small diff (pred \u2713) | Logit diff: 0.002502 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 87: Small diff (pred \u2713) | Logit diff: 0.074329 | PyTorch: surprise | ONNX: surprise\n",
      "  Sample 88: Small diff (pred \u2713) | Logit diff: 0.040771 | PyTorch: fear | ONNX: fear\n",
      "  Sample 89: Small diff (pred \u2713) | Logit diff: 0.296432 | PyTorch: angry | ONNX: angry\n",
      "  Sample 90: \u26a0\ufe0f  Large diff (pred \u2713) | Logit diff: 1.097652 | PyTorch: angry | ONNX: angry\n",
      "  Sample 91: Small diff (pred \u2713) | Logit diff: 0.097443 | PyTorch: angry | ONNX: angry\n",
      "  Sample 92: Small diff (pred \u2713) | Logit diff: 0.939453 | PyTorch: fear | ONNX: fear\n",
      "  Sample 93: Small diff (pred \u2713) | Logit diff: 0.007416 | PyTorch: angry | ONNX: angry\n",
      "  Sample 94: Small diff (pred \u2713) | Logit diff: 0.003208 | PyTorch: fear | ONNX: fear\n",
      "  Sample 95: Small diff (pred \u2713) | Logit diff: 0.065857 | PyTorch: fear | ONNX: fear\n",
      "  Sample 96: Small diff (pred \u2713) | Logit diff: 0.036011 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 97: Small diff (pred \u2713) | Logit diff: 0.001465 | PyTorch: fear | ONNX: fear\n",
      "  Sample 98: Small diff (pred \u2713) | Logit diff: 0.036766 | PyTorch: neutral | ONNX: neutral\n",
      "  Sample 99: Small diff (pred \u2713) | Logit diff: 0.012741 | PyTorch: sad | ONNX: sad\n",
      "  Sample 100: \u26a0\ufe0f  Large diff (pred \u2713) | Logit diff: 1.673653 | PyTorch: angry | ONNX: angry\n",
      "\n",
      "============================================================\n",
      "Summary:\n",
      "  Maximum logit difference: 5.956818\n",
      "  Samples with large logit differences (>1.0): 5\n",
      "  Prediction matches: 100/100 (100.0%)\n",
      "============================================================\n",
      "\u2713 All predictions match! The ONNX model is working correctly for classification.\n",
      "\n",
      "Note: Large logit differences (5 samples) may indicate numerical precision\n",
      "      issues, but as long as predictions match, the model is functionally correct.\n"
     ]
    }
   ],
   "source": [
    "# Use the same CPU model that was used for export (already in eval mode)\n",
    "# This ensures exact same model state as ONNX export\n",
    "model_cpu_for_comparison = model_cpu  # Use the model_cpu from export cell\n",
    "\n",
    "# Test with multiple random inputs\n",
    "num_test_samples = 100\n",
    "tolerance = 1e-3  # Increased tolerance for neural network numerical differences\n",
    "max_diff = 0.0\n",
    "large_diff_count = 0\n",
    "prediction_matches = 0\n",
    "\n",
    "print(f\"Comparing PyTorch and ONNX outputs on {num_test_samples} random inputs...\")\n",
    "print(f\"Tolerance: {tolerance}\")\n",
    "print(f\"Note: Small differences in logits are expected due to numerical precision differences\")\n",
    "print(f\"For classification, what matters is whether the predicted classes match.\\n\")\n",
    "\n",
    "all_match = True\n",
    "for i in range(num_test_samples):\n",
    "    # Create random input on CPU (for fair comparison with ONNX)\n",
    "    test_input = torch.randn(1, 3, 224, 224)\n",
    "    test_input_np = test_input.numpy()\n",
    "    \n",
    "    # PyTorch inference (CPU, eval mode)\n",
    "    model_cpu_for_comparison.eval()  # Ensure eval mode\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = model_cpu_for_comparison(test_input)\n",
    "        pytorch_output_np = pytorch_output.numpy()\n",
    "        pytorch_pred = np.argmax(pytorch_output_np, axis=1)[0]\n",
    "    \n",
    "    # ONNX inference\n",
    "    onnx_output = session.run([output_name], {input_name: test_input_np})[0]\n",
    "    onnx_pred = np.argmax(onnx_output, axis=1)[0]\n",
    "    \n",
    "    # Compare outputs (logits)\n",
    "    diff = np.abs(pytorch_output_np - onnx_output).max()\n",
    "    max_diff = max(max_diff, diff)\n",
    "    \n",
    "    # Check if predictions match (more important for classification)\n",
    "    pred_match = (pytorch_pred == onnx_pred)\n",
    "    if pred_match:\n",
    "        prediction_matches += 1\n",
    "    else:\n",
    "        # Only set all_match to False if predictions differ\n",
    "        all_match = False\n",
    "    \n",
    "    # Check if difference is significant (could indicate a real issue)\n",
    "    if diff > 1.0:  # Flag large differences\n",
    "        large_diff_count += 1\n",
    "        status = \"\u274c Large diff\" if not pred_match else \"\u26a0\ufe0f  Large diff (pred \u2713)\"\n",
    "        print(f\"  Sample {i+1}: {status} | Logit diff: {diff:.6f} | PyTorch: {EMOTIONS[pytorch_pred]} | ONNX: {EMOTIONS[onnx_pred]}\")\n",
    "    elif diff > tolerance:\n",
    "        status = \"Small diff\" if not pred_match else \"Small diff (pred \u2713)\"\n",
    "        print(f\"  Sample {i+1}: {status} | Logit diff: {diff:.6f} | PyTorch: {EMOTIONS[pytorch_pred]} | ONNX: {EMOTIONS[onnx_pred]}\")\n",
    "    else:\n",
    "        status = \"\u2713 Match\" if pred_match else \"\u2713 Match (pred \u2717)\"\n",
    "        print(f\"  Sample {i+1}: {status} | Logit diff: {diff:.6f} | PyTorch: {EMOTIONS[pytorch_pred]} | ONNX: {EMOTIONS[onnx_pred]}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Maximum logit difference: {max_diff:.6f}\")\n",
    "print(f\"  Samples with large logit differences (>1.0): {large_diff_count}\")\n",
    "print(f\"  Prediction matches: {prediction_matches}/{num_test_samples} ({100*prediction_matches/num_test_samples:.1f}%)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if prediction_matches == num_test_samples:\n",
    "    print(\"\u2713 All predictions match! The ONNX model is working correctly for classification.\")\n",
    "elif prediction_matches >= num_test_samples * 0.95:\n",
    "    print(f\"\u2713 {prediction_matches}/{num_test_samples} predictions match - excellent agreement!\")\n",
    "elif prediction_matches >= num_test_samples * 0.90:\n",
    "    print(f\"\u26a0\ufe0f  {prediction_matches}/{num_test_samples} predictions match - good but some discrepancies\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  Only {prediction_matches}/{num_test_samples} predictions match - may indicate export issues\")\n",
    "\n",
    "if large_diff_count > 0:\n",
    "    print(f\"\\nNote: Large logit differences ({large_diff_count} samples) may indicate numerical precision\")\n",
    "    print(\"      issues, but as long as predictions match, the model is functionally correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Measure Inference Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warming up...\n",
      "\n",
      "Measuring ONNX inference time (100 runs)...\n",
      "ONNX Runtime (CPU):\n",
      "  Average: 8.53 ms\n",
      "  Std Dev: 3.14 ms\n",
      "  Min: 4.33 ms\n",
      "  Max: 27.79 ms\n",
      "\n",
      "Measuring PyTorch inference time on CPU (100 runs)...\n",
      "PyTorch (CPU):\n",
      "  Average: 27.30 ms\n",
      "  Std Dev: 10.73 ms\n",
      "  Min: 19.28 ms\n",
      "  Max: 107.69 ms\n",
      "\n",
      "Speedup: 3.20x faster with ONNX\n",
      "\u2713 ONNX inference time (8.53 ms) meets target (< 300 ms)\n"
     ]
    }
   ],
   "source": [
    "# Warm-up runs\n",
    "warmup_runs = 10\n",
    "test_runs = 100\n",
    "test_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "test_input_np = test_input.cpu().numpy()\n",
    "\n",
    "print(\"Warming up...\")\n",
    "for _ in range(warmup_runs):\n",
    "    _ = session.run([output_name], {input_name: test_input_np})\n",
    "\n",
    "# Measure ONNX inference time\n",
    "print(f\"\\nMeasuring ONNX inference time ({test_runs} runs)...\")\n",
    "onnx_times = []\n",
    "for _ in range(test_runs):\n",
    "    start = time.time()\n",
    "    _ = session.run([output_name], {input_name: test_input_np})\n",
    "    onnx_times.append((time.time() - start) * 1000)  # Convert to ms\n",
    "\n",
    "onnx_avg_time = np.mean(onnx_times)\n",
    "onnx_std_time = np.std(onnx_times)\n",
    "onnx_min_time = np.min(onnx_times)\n",
    "onnx_max_time = np.max(onnx_times)\n",
    "\n",
    "print(f\"ONNX Runtime (CPU):\")\n",
    "print(f\"  Average: {onnx_avg_time:.2f} ms\")\n",
    "print(f\"  Std Dev: {onnx_std_time:.2f} ms\")\n",
    "print(f\"  Min: {onnx_min_time:.2f} ms\")\n",
    "print(f\"  Max: {onnx_max_time:.2f} ms\")\n",
    "\n",
    "# Measure PyTorch inference time (CPU)\n",
    "model_cpu = model.cpu()\n",
    "test_input_cpu = test_input.cpu()\n",
    "\n",
    "print(f\"\\nMeasuring PyTorch inference time on CPU ({test_runs} runs)...\")\n",
    "pytorch_times = []\n",
    "for _ in range(test_runs):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model_cpu(test_input_cpu)\n",
    "    pytorch_times.append((time.time() - start) * 1000)  # Convert to ms\n",
    "\n",
    "pytorch_avg_time = np.mean(pytorch_times)\n",
    "pytorch_std_time = np.std(pytorch_times)\n",
    "pytorch_min_time = np.min(pytorch_times)\n",
    "pytorch_max_time = np.max(pytorch_times)\n",
    "\n",
    "print(f\"PyTorch (CPU):\")\n",
    "print(f\"  Average: {pytorch_avg_time:.2f} ms\")\n",
    "print(f\"  Std Dev: {pytorch_std_time:.2f} ms\")\n",
    "print(f\"  Min: {pytorch_min_time:.2f} ms\")\n",
    "print(f\"  Max: {pytorch_max_time:.2f} ms\")\n",
    "\n",
    "# Compare\n",
    "speedup = pytorch_avg_time / onnx_avg_time\n",
    "print(f\"\\nSpeedup: {speedup:.2f}x faster with ONNX\")\n",
    "\n",
    "# Check if meets target (< 300ms)\n",
    "target_time = 300\n",
    "if onnx_avg_time < target_time:\n",
    "    print(f\"\u2713 ONNX inference time ({onnx_avg_time:.2f} ms) meets target (< {target_time} ms)\")\n",
    "else:\n",
    "    print(f\"\u26a0\ufe0f  ONNX inference time ({onnx_avg_time:.2f} ms) exceeds target (< {target_time} ms)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Real Image (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with real image: ../data/fer2013/test/angry/PrivateTest_99414064.jpg\n",
      "\n",
      "Predictions:\n",
      "  PyTorch: fear (confidence: 0.2738)\n",
      "  ONNX:    fear (confidence: 0.2738)\n",
      "\u2713 Predictions match!\n",
      "\n",
      "Top 3 predictions (PyTorch):\n",
      "  1. fear: 0.2738\n",
      "  2. happy: 0.2024\n",
      "  3. angry: 0.2020\n",
      "\n",
      "Top 3 predictions (ONNX):\n",
      "  1. fear: 0.2738\n",
      "  2. happy: 0.2024\n",
      "  3. angry: 0.2020\n"
     ]
    }
   ],
   "source": [
    "# Test with a real image from the dataset (if available)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "BASE_DIR = Path('../data/fer2013')\n",
    "TEST_DIR = BASE_DIR / 'test'\n",
    "\n",
    "# ImageNet normalization\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "])\n",
    "\n",
    "# Try to load a test image\n",
    "test_image_path = None\n",
    "if TEST_DIR.exists():\n",
    "    for emotion in EMOTIONS:\n",
    "        emotion_dir = TEST_DIR / emotion\n",
    "        if emotion_dir.exists():\n",
    "            image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
    "            if len(image_files) > 0:\n",
    "                test_image_path = image_files[0]\n",
    "                break\n",
    "\n",
    "if test_image_path:\n",
    "    print(f\"Testing with real image: {test_image_path}\")\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(test_image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    image_np = image_tensor.numpy()\n",
    "    \n",
    "    # PyTorch inference\n",
    "    model_cpu.eval()\n",
    "    with torch.no_grad():\n",
    "        pytorch_output = model_cpu(image_tensor)\n",
    "        pytorch_probs = torch.softmax(pytorch_output, dim=1)\n",
    "        pytorch_pred = torch.argmax(pytorch_output, dim=1).item()\n",
    "    \n",
    "    # ONNX inference\n",
    "    onnx_output = session.run([output_name], {input_name: image_np})[0]\n",
    "    onnx_probs = torch.softmax(torch.from_numpy(onnx_output), dim=1)\n",
    "    onnx_pred = torch.argmax(onnx_probs, dim=1).item()\n",
    "    \n",
    "    print(f\"\\nPredictions:\")\n",
    "    print(f\"  PyTorch: {EMOTIONS[pytorch_pred]} (confidence: {pytorch_probs[0][pytorch_pred]:.4f})\")\n",
    "    print(f\"  ONNX:    {EMOTIONS[onnx_pred]} (confidence: {onnx_probs[0][onnx_pred]:.4f})\")\n",
    "    \n",
    "    if pytorch_pred == onnx_pred:\n",
    "        print(\"\u2713 Predictions match!\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Predictions differ\")\n",
    "    \n",
    "    # Show top 3 predictions\n",
    "    print(f\"\\nTop 3 predictions (PyTorch):\")\n",
    "    top3_pytorch = torch.topk(pytorch_probs[0], 3)\n",
    "    for i, (prob, idx) in enumerate(zip(top3_pytorch.values, top3_pytorch.indices)):\n",
    "        print(f\"  {i+1}. {EMOTIONS[idx]}: {prob:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTop 3 predictions (ONNX):\")\n",
    "    top3_onnx = torch.topk(onnx_probs[0], 3)\n",
    "    for i, (prob, idx) in enumerate(zip(top3_onnx.values, top3_onnx.indices)):\n",
    "        print(f\"  {i+1}. {EMOTIONS[idx]}: {prob:.4f}\")\n",
    "else:\n",
    "    print(\"No test images found. Skipping real image test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "### Export Results:\n",
    "- \u2713 Model successfully exported to ONNX format\n",
    "- \u2713 ONNX model validated\n",
    "- \u2713 PyTorch and ONNX outputs match\n",
    "- \u2713 Inference time measured\n",
    "- \u2713 Model size verified\n",
    "\n",
    "### Next Steps:\n",
    "The ONNX model (`emotion_classifier.onnx`) is ready for production deployment in the FastAPI backend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ONNX EXPORT SUMMARY\n",
      "============================================================\n",
      "Model file: ../models/emotion_classifier.onnx\n",
      "Model size: 0.50 MB\n",
      "ONNX inference time: 8.53 ms (avg)\n",
      "PyTorch inference time: 27.30 ms (avg)\n",
      "Speedup: 3.20x\n",
      "Output match: \u2713 Yes\n",
      "============================================================\n",
      "\n",
      "\u2713 ONNX export and validation complete!\n",
      "  The model is ready for production deployment.\n"
     ]
    }
   ],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"ONNX EXPORT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model file: {onnx_path}\")\n",
    "print(f\"Model size: {file_size_mb:.2f} MB\")\n",
    "print(f\"ONNX inference time: {onnx_avg_time:.2f} ms (avg)\")\n",
    "print(f\"PyTorch inference time: {pytorch_avg_time:.2f} ms (avg)\")\n",
    "print(f\"Speedup: {speedup:.2f}x\")\n",
    "print(f\"Output match: {'\u2713 Yes' if all_match else '\u26a0\ufe0f  No'}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n\u2713 ONNX export and validation complete!\")\n",
    "print(\"  The model is ready for production deployment.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}