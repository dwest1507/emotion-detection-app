{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Emotion Classification: Model Training with Transfer Learning\n",
        "\n",
        "This notebook implements the training pipeline for emotion classification using EfficientNet-B0 with transfer learning on the FER-2013 dataset.\n",
        "\n",
        "## Model Architecture\n",
        "- **Base Model**: EfficientNet-B0 (pre-trained on ImageNet)\n",
        "- **Approach**: Transfer learning with fine-tuning\n",
        "- **Input**: 224×224 RGB images (converted from 48×48 grayscale)\n",
        "- **Output**: 7-class emotion classification\n",
        "- **Framework**: PyTorch → ONNX export for production\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import models, transforms\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Device Setup and GPU Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# GPU information\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
        "    # Enable cuDNN benchmark for faster training\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    print(\"cuDNN benchmark enabled\")\n",
        "else:\n",
        "    print(\"CUDA not available. Training will be slower on CPU.\")\n",
        "\n",
        "# Set number of classes\n",
        "NUM_CLASSES = 7\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(EMOTIONS)}\n",
        "IDX_TO_EMOTION = {idx: emotion for emotion, idx in EMOTION_TO_IDX.items()}\n",
        "\n",
        "print(f\"\\nNumber of classes: {NUM_CLASSES}\")\n",
        "print(f\"Emotion classes: {EMOTIONS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Paths and Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "BASE_DIR = Path('../data/fer2013')\n",
        "TRAIN_DIR = BASE_DIR / 'train'\n",
        "TEST_DIR = BASE_DIR / 'test'\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS_PHASE1 = 3  # Frozen backbone\n",
        "NUM_EPOCHS_PHASE2 = 5  # Fine-tuning\n",
        "VALIDATION_SPLIT = 0.1  # 10% of training data for validation\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Train directory exists: {TRAIN_DIR.exists()}\")\n",
        "print(f\"Test directory exists: {TEST_DIR.exists()}\")\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
        "print(f\"  Phase 1 epochs (frozen): {NUM_EPOCHS_PHASE1}\")\n",
        "print(f\"  Phase 2 epochs (fine-tune): {NUM_EPOCHS_PHASE2}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Custom Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FER2013Dataset(Dataset):\n",
        "    \"\"\"Custom Dataset class for FER-2013 dataset.\"\"\"\n",
        "    \n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Path to directory containing emotion subdirectories\n",
        "            transform: Optional transform to be applied on a sample\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        \n",
        "        # Load all image paths with their labels\n",
        "        for emotion in EMOTIONS:\n",
        "            emotion_dir = self.data_dir / emotion\n",
        "            if emotion_dir.exists():\n",
        "                image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
        "                for img_path in image_files:\n",
        "                    self.samples.append((img_path, EMOTION_TO_IDX[emotion]))\n",
        "        \n",
        "        print(f\"Loaded {len(self.samples)} samples from {data_dir}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        \n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')  # Convert grayscale to RGB\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            image = Image.new('RGB', (48, 48), color='black')\n",
        "        \n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "# Test dataset class\n",
        "if TRAIN_DIR.exists():\n",
        "    test_dataset = FER2013Dataset(TRAIN_DIR, transform=None)\n",
        "    print(f\"\\nDataset test successful! Found {len(test_dataset)} samples\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Dataset not found. Please download the dataset first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data Transforms and Augmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ImageNet normalization statistics (for pre-trained models)\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Training transforms with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize from 48x48 to 224x224\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "# Validation/Test transforms (no augmentation)\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "])\n",
        "\n",
        "print(\"Transforms defined:\")\n",
        "print(\"\\nTraining transforms:\")\n",
        "print(train_transform)\n",
        "print(\"\\nValidation/Test transforms:\")\n",
        "print(val_test_transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "if TRAIN_DIR.exists() and TEST_DIR.exists():\n",
        "    # Training dataset\n",
        "    train_dataset_full = FER2013Dataset(TRAIN_DIR, transform=train_transform)\n",
        "    \n",
        "    # Split training data into train and validation\n",
        "    train_size = int((1 - VALIDATION_SPLIT) * len(train_dataset_full))\n",
        "    val_size = len(train_dataset_full) - train_size\n",
        "    train_dataset, val_dataset = random_split(\n",
        "        train_dataset_full, \n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(42)\n",
        "    )\n",
        "    \n",
        "    # Update validation dataset transforms (no augmentation)\n",
        "    val_dataset.dataset = FER2013Dataset(TRAIN_DIR, transform=val_test_transform)\n",
        "    val_indices = val_dataset.indices\n",
        "    val_dataset = torch.utils.data.Subset(\n",
        "        FER2013Dataset(TRAIN_DIR, transform=val_test_transform),\n",
        "        val_indices\n",
        "    )\n",
        "    \n",
        "    # Test dataset\n",
        "    test_dataset = FER2013Dataset(TEST_DIR, transform=val_test_transform)\n",
        "    \n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, \n",
        "        batch_size=BATCH_SIZE, \n",
        "        shuffle=True, \n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "    \n",
        "    print(f\"DataLoaders created:\")\n",
        "    print(f\"  Train batches: {len(train_loader)} ({len(train_dataset)} samples)\")\n",
        "    print(f\"  Validation batches: {len(val_loader)} ({len(val_dataset)} samples)\")\n",
        "    print(f\"  Test batches: {len(test_loader)} ({len(test_dataset)} samples)\")\n",
        "else:\n",
        "    print(\"⚠️  Dataset directories not found. Please download the dataset first.\")\n",
        "    train_loader = None\n",
        "    val_loader = None\n",
        "    test_loader = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Architecture: EfficientNet-B0 with Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(num_classes=7, freeze_backbone=True):\n",
        "    \"\"\"\n",
        "    Create EfficientNet-B0 model with transfer learning.\n",
        "    \n",
        "    Args:\n",
        "        num_classes: Number of output classes (7 for FER-2013)\n",
        "        freeze_backbone: If True, freeze feature extraction layers\n",
        "    \n",
        "    Returns:\n",
        "        model: PyTorch model\n",
        "    \"\"\"\n",
        "    # Load pre-trained EfficientNet-B0\n",
        "    model = models.efficientnet_b0(weights='IMAGENET1K_V1')\n",
        "    \n",
        "    # Freeze backbone layers (Phase 1 training)\n",
        "    if freeze_backbone:\n",
        "        for param in model.features.parameters():\n",
        "            param.requires_grad = False\n",
        "        print(\"Backbone layers frozen\")\n",
        "    else:\n",
        "        print(\"All layers trainable (fine-tuning)\")\n",
        "    \n",
        "    # Replace classifier head for our number of classes\n",
        "    # EfficientNet-B0 classifier structure: Dropout -> Linear\n",
        "    in_features = model.classifier[1].in_features\n",
        "    model.classifier = nn.Sequential(\n",
        "        nn.Dropout(p=0.3),\n",
        "        nn.Linear(in_features, num_classes)\n",
        "    )\n",
        "    \n",
        "    print(f\"Model created: EfficientNet-B0\")\n",
        "    print(f\"  Input features: {in_features}\")\n",
        "    print(f\"  Output classes: {num_classes}\")\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_model(num_classes=NUM_CLASSES, freeze_backbone=True)\n",
        "model = model.to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nModel parameters:\")\n",
        "print(f\"  Total: {total_params:,}\")\n",
        "print(f\"  Trainable: {trainable_params:,}\")\n",
        "print(f\"  Frozen: {total_params - trainable_params:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Training Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights for imbalanced dataset (optional)\n",
        "# This helps with the class imbalance issue (especially Disgust class)\n",
        "def calculate_class_weights(dataset):\n",
        "    \"\"\"Calculate class weights based on class frequencies.\"\"\"\n",
        "    class_counts = torch.zeros(NUM_CLASSES)\n",
        "    \n",
        "    for _, label in dataset:\n",
        "        class_counts[label] += 1\n",
        "    \n",
        "    # Calculate weights (inverse frequency)\n",
        "    total = class_counts.sum()\n",
        "    class_weights = total / (NUM_CLASSES * class_counts)\n",
        "    class_weights = class_weights / class_weights.sum() * NUM_CLASSES  # Normalize\n",
        "    \n",
        "    return class_weights\n",
        "\n",
        "# Calculate class weights if dataset is available\n",
        "if train_loader is not None:\n",
        "    # Get full training dataset for weight calculation\n",
        "    full_train_dataset = FER2013Dataset(TRAIN_DIR, transform=None)\n",
        "    class_weights = calculate_class_weights(full_train_dataset)\n",
        "    class_weights = class_weights.to(device)\n",
        "    print(\"Class weights calculated:\")\n",
        "    for i, emotion in enumerate(EMOTIONS):\n",
        "        print(f\"  {emotion:10s}: {class_weights[i]:.4f}\")\n",
        "else:\n",
        "    class_weights = None\n",
        "    print(\"Class weights not calculated (dataset not available)\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Mixed precision training scaler\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  Loss function: CrossEntropyLoss (with class weights: {class_weights is not None})\")\n",
        "print(f\"  Optimizer: AdamW (lr={LEARNING_RATE}, weight_decay={WEIGHT_DECAY})\")\n",
        "print(f\"  Scheduler: ReduceLROnPlateau (factor=0.5, patience=2)\")\n",
        "print(f\"  Mixed precision: Enabled\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, criterion, optimizer, device, scaler):\n",
        "    \"\"\"Train for one epoch.\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader, desc='Training')\n",
        "    for images, labels in progress_bar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass with mixed precision\n",
        "        with autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward pass with scaler\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # Statistics\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        # Update progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': f'{loss.item():.4f}',\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Validate for one epoch.\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(dataloader, desc='Validation')\n",
        "        for images, labels in progress_bar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Statistics\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            \n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'acc': f'{100 * correct / total:.2f}%'\n",
        "            })\n",
        "    \n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    epoch_acc = 100 * correct / total\n",
        "    \n",
        "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
        "\n",
        "print(\"Training functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Loop Structure\n",
        "\n",
        "The training will be done in two phases:\n",
        "1. **Phase 1**: Train only classifier head (backbone frozen) - 3 epochs\n",
        "2. **Phase 2**: Fine-tune entire model (all layers trainable) - 5 epochs\n",
        "\n",
        "### Phase 1: Frozen Backbone Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "if train_loader is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"PHASE 1: Training Classifier Head (Backbone Frozen)\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS_PHASE1):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS_PHASE1}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, scaler\n",
        "        )\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # Print epoch results\n",
        "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '../models/best_model_phase1.pth')\n",
        "            print(\"  ✓ Best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"  Early stopping triggered after {epoch + 1} epochs\")\n",
        "                break\n",
        "else:\n",
        "    print(\"⚠️  Cannot train: DataLoaders not available. Please download dataset first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Phase 2: Fine-tuning (All Layers Trainable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unfreeze all layers for fine-tuning\n",
        "if train_loader is not None:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PHASE 2: Fine-tuning All Layers\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Unfreeze backbone\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    # Use lower learning rate for fine-tuning\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE * 0.1,  # 10x lower learning rate\n",
        "        weight_decay=WEIGHT_DECAY\n",
        "    )\n",
        "    \n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=0.5,\n",
        "        patience=2,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Reset early stopping\n",
        "    best_val_loss = min(history['val_loss']) if history['val_loss'] else float('inf')\n",
        "    patience_counter = 0\n",
        "    \n",
        "    print(f\"Unfroze all layers. Fine-tuning with lr={optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    \n",
        "    for epoch in range(NUM_EPOCHS_PHASE2):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS_PHASE2}\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device, scaler\n",
        "        )\n",
        "        \n",
        "        # Validate\n",
        "        val_loss, val_acc, _, _ = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Update learning rate\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "        \n",
        "        # Print epoch results\n",
        "        print(f\"\\nEpoch {epoch + 1} Results:\")\n",
        "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), '../models/best_model_phase2.pth')\n",
        "            print(\"  ✓ Best model saved!\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f\"  Early stopping triggered after {epoch + 1} epochs\")\n",
        "                break\n",
        "else:\n",
        "    print(\"⚠️  Cannot fine-tune: DataLoaders not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Training Curves Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training curves\n",
        "if len(history['train_loss']) > 0:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    epochs = range(1, len(history['train_loss']) + 1)\n",
        "    \n",
        "    # Loss plot\n",
        "    axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "    axes[0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Accuracy plot\n",
        "    axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "    axes[1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
        "    axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"  Best Train Accuracy: {max(history['train_acc']):.2f}%\")\n",
        "    print(f\"  Best Validation Accuracy: {max(history['val_acc']):.2f}%\")\n",
        "    print(f\"  Best Validation Loss: {min(history['val_loss']):.4f}\")\n",
        "else:\n",
        "    print(\"No training history available. Run training cells first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Evaluation on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model and evaluate on test set\n",
        "if test_loader is not None:\n",
        "    print(\"=\" * 60)\n",
        "    print(\"EVALUATION ON TEST SET\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Load best model\n",
        "    try:\n",
        "        model.load_state_dict(torch.load('../models/best_model_phase2.pth'))\n",
        "        print(\"Loaded best model from Phase 2\")\n",
        "    except:\n",
        "        try:\n",
        "            model.load_state_dict(torch.load('../models/best_model_phase1.pth'))\n",
        "            print(\"Loaded best model from Phase 1\")\n",
        "        except:\n",
        "            print(\"Using current model state\")\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    # Evaluate\n",
        "    test_loss, test_acc, test_preds, test_labels = validate_epoch(\n",
        "        model, test_loader, criterion, device\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
        "    print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
        "    \n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(test_labels, test_preds)\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=EMOTIONS, yticklabels=EMOTIONS)\n",
        "    plt.title('Confusion Matrix - Test Set', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, test_preds, \n",
        "                              target_names=EMOTIONS, digits=4))\n",
        "    \n",
        "    # Per-class accuracy\n",
        "    print(\"\\nPer-Class Accuracy:\")\n",
        "    for i, emotion in enumerate(EMOTIONS):\n",
        "        class_mask = np.array(test_labels) == i\n",
        "        if class_mask.sum() > 0:\n",
        "            class_acc = (np.array(test_preds)[class_mask] == i).sum() / class_mask.sum() * 100\n",
        "            print(f\"  {emotion:10s}: {class_acc:.2f}%\")\n",
        "else:\n",
        "    print(\"⚠️  Cannot evaluate: Test DataLoader not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Final Model\n",
        "\n",
        "The model will be exported to ONNX format in the next notebook (03_onnx_validation.ipynb) for production deployment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save final model state\n",
        "if test_loader is not None:\n",
        "    # Create models directory if it doesn't exist\n",
        "    Path('../models').mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save PyTorch model\n",
        "    torch.save(model.state_dict(), '../models/final_model.pth')\n",
        "    print(\"Final model saved to ../models/final_model.pth\")\n",
        "    \n",
        "    # Save model architecture info\n",
        "    model_info = {\n",
        "        'num_classes': NUM_CLASSES,\n",
        "        'emotions': EMOTIONS,\n",
        "        'emotion_to_idx': EMOTION_TO_IDX,\n",
        "        'idx_to_emotion': IDX_TO_EMOTION,\n",
        "        'test_accuracy': test_acc if 'test_acc' in locals() else None\n",
        "    }\n",
        "    \n",
        "    import json\n",
        "    with open('../models/model_info.json', 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "    print(\"Model info saved to ../models/model_info.json\")\n",
        "    \n",
        "    print(\"\\n✓ Model training pipeline complete!\")\n",
        "    print(\"  Next step: Export to ONNX format (see 03_onnx_validation.ipynb)\")\n",
        "else:\n",
        "    print(\"⚠️  Cannot save model: Evaluation not completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
