{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FER-2013 Dataset: Exploratory Data Analysis\n",
        "\n",
        "This notebook performs exploratory data analysis on the FER-2013 (Facial Expression Recognition 2013) dataset.\n",
        "\n",
        "## Dataset Information\n",
        "- **Source**: [Kaggle - FER-2013](https://www.kaggle.com/datasets/msambare/fer2013)\n",
        "- **Size**: 35,887 grayscale images (48×48 pixels)\n",
        "- **Classes**: 7 emotions (Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral)\n",
        "- **Citation**: Goodfellow, I. J., et al. (2013). Challenges in representation learning: A report on three machine learning contests. *Neural Networks*, 64, 59-63.\n",
        "\n",
        "## Download Instructions\n",
        "\n",
        "To download the dataset, run the following command in your terminal:\n",
        "\n",
        "```bash\n",
        "# Install kaggle package if not already installed\n",
        "pip install kaggle\n",
        "\n",
        "# Download the dataset (requires Kaggle API credentials in ~/.kaggle/kaggle.json)\n",
        "kaggle datasets download -d msambare/fer2013\n",
        "\n",
        "# Extract to data/fer2013/\n",
        "# The dataset should have the following structure:\n",
        "# data/fer2013/\n",
        "#   ├── train/\n",
        "#   │   ├── angry/\n",
        "#   │   ├── disgust/\n",
        "#   │   ├── fear/\n",
        "#   │   ├── happy/\n",
        "#   │   ├── sad/\n",
        "#   │   ├── surprise/\n",
        "#   │   └── neutral/\n",
        "#   └── test/\n",
        "#       ├── angry/\n",
        "#       ├── disgust/\n",
        "#       ├── fear/\n",
        "#       ├── happy/\n",
        "#       ├── sad/\n",
        "#       ├── surprise/\n",
        "#       └── neutral/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Paths and Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define dataset paths\n",
        "BASE_DIR = Path('../data/fer2013')\n",
        "TRAIN_DIR = BASE_DIR / 'train'\n",
        "TEST_DIR = BASE_DIR / 'test'\n",
        "\n",
        "# Emotion classes (7 classes)\n",
        "EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
        "\n",
        "print(f\"Base directory: {BASE_DIR}\")\n",
        "print(f\"Train directory exists: {TRAIN_DIR.exists()}\")\n",
        "print(f\"Test directory exists: {TEST_DIR.exists()}\")\n",
        "print(f\"\\nEmotion classes: {EMOTIONS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Dataset Validation and Basic Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count images per class in train and test sets\n",
        "def count_images_per_class(directory, emotions):\n",
        "    \"\"\"Count images in each emotion class directory.\"\"\"\n",
        "    counts = {}\n",
        "    for emotion in emotions:\n",
        "        emotion_dir = directory / emotion\n",
        "        if emotion_dir.exists():\n",
        "            # Count image files\n",
        "            image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
        "            counts[emotion] = len(image_files)\n",
        "        else:\n",
        "            counts[emotion] = 0\n",
        "    return counts\n",
        "\n",
        "# Get counts for train and test sets\n",
        "train_counts = count_images_per_class(TRAIN_DIR, EMOTIONS)\n",
        "test_counts = count_images_per_class(TEST_DIR, EMOTIONS)\n",
        "\n",
        "# Create DataFrame for better visualization\n",
        "df_train = pd.DataFrame(list(train_counts.items()), columns=['Emotion', 'Train_Count'])\n",
        "df_test = pd.DataFrame(list(test_counts.items()), columns=['Emotion', 'Test_Count'])\n",
        "df_counts = pd.merge(df_train, df_test, on='Emotion')\n",
        "df_counts['Total'] = df_counts['Train_Count'] + df_counts['Test_Count']\n",
        "\n",
        "print(\"Image counts per class:\")\n",
        "print(df_counts.to_string(index=False))\n",
        "print(f\"\\nTotal train images: {df_counts['Train_Count'].sum()}\")\n",
        "print(f\"Total test images: {df_counts['Test_Count'].sum()}\")\n",
        "print(f\"Total images: {df_counts['Total'].sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Class Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Train set distribution\n",
        "axes[0].bar(df_counts['Emotion'], df_counts['Train_Count'], color='steelblue')\n",
        "axes[0].set_title('Train Set: Class Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Emotion', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Images', fontsize=12)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(df_counts['Train_Count']):\n",
        "    axes[0].text(i, v + 50, str(v), ha='center', va='bottom')\n",
        "\n",
        "# Test set distribution\n",
        "axes[1].bar(df_counts['Emotion'], df_counts['Test_Count'], color='coral')\n",
        "axes[1].set_title('Test Set: Class Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Emotion', fontsize=12)\n",
        "axes[1].set_ylabel('Number of Images', fontsize=12)\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, v in enumerate(df_counts['Test_Count']):\n",
        "    axes[1].text(i, v + 20, str(v), ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class imbalance ratios\n",
        "total_counts = df_counts['Total'].values\n",
        "max_count = total_counts.max()\n",
        "min_count = total_counts.min()\n",
        "imbalance_ratios = max_count / total_counts\n",
        "\n",
        "print(\"Class Imbalance Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "for i, emotion in enumerate(EMOTIONS):\n",
        "    ratio = imbalance_ratios[i]\n",
        "    print(f\"{emotion.capitalize():10s}: {total_counts[i]:5d} images | Imbalance ratio: {ratio:.2f}x\")\n",
        "\n",
        "print(f\"\\nMost imbalanced: {max_count / min_count:.2f}x difference\")\n",
        "print(f\"\\n⚠️  Note: 'Disgust' class has only {df_counts[df_counts['Emotion']=='disgust']['Total'].values[0]} images\")\n",
        "print(\"   This is significantly fewer than other classes and may need special handling.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Quality Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample images from each class\n",
        "def display_samples(directory, emotions, num_samples=5):\n",
        "    \"\"\"Display sample images from each emotion class.\"\"\"\n",
        "    fig, axes = plt.subplots(len(emotions), num_samples, figsize=(15, 18))\n",
        "    \n",
        "    for i, emotion in enumerate(emotions):\n",
        "        emotion_dir = directory / emotion\n",
        "        if emotion_dir.exists():\n",
        "            # Get random sample of images\n",
        "            image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
        "            if len(image_files) > 0:\n",
        "                sample_files = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
        "                \n",
        "                for j, img_path in enumerate(sample_files):\n",
        "                    try:\n",
        "                        img = Image.open(img_path)\n",
        "                        axes[i, j].imshow(img, cmap='gray')\n",
        "                        axes[i, j].set_title(f'{emotion.capitalize()}\\n{img.size}', fontsize=10)\n",
        "                        axes[i, j].axis('off')\n",
        "                    except Exception as e:\n",
        "                        axes[i, j].text(0.5, 0.5, f'Error\\n{str(e)[:20]}', \n",
        "                                       ha='center', va='center', transform=axes[i, j].transAxes)\n",
        "                        axes[i, j].axis('off')\n",
        "            else:\n",
        "                for j in range(num_samples):\n",
        "                    axes[i, j].text(0.5, 0.5, 'No images', ha='center', va='center')\n",
        "                    axes[i, j].axis('off')\n",
        "        else:\n",
        "            for j in range(num_samples):\n",
        "                axes[i, j].text(0.5, 0.5, 'Directory not found', ha='center', va='center')\n",
        "                axes[i, j].axis('off')\n",
        "    \n",
        "    plt.suptitle(f'Sample Images from {directory.name.capitalize()} Set', fontsize=16, fontweight='bold', y=0.995)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display samples from train set\n",
        "print(\"Sample images from TRAIN set:\")\n",
        "display_samples(TRAIN_DIR, EMOTIONS, num_samples=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check image dimensions and format\n",
        "def check_image_properties(directory, emotions, sample_size=100):\n",
        "    \"\"\"Check properties of images in the dataset.\"\"\"\n",
        "    dimensions = []\n",
        "    formats = []\n",
        "    corrupted = []\n",
        "    \n",
        "    for emotion in emotions:\n",
        "        emotion_dir = directory / emotion\n",
        "        if emotion_dir.exists():\n",
        "            image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
        "            # Sample images for faster processing\n",
        "            sample_files = np.random.choice(image_files, min(sample_size, len(image_files)), replace=False)\n",
        "            \n",
        "            for img_path in sample_files:\n",
        "                try:\n",
        "                    img = Image.open(img_path)\n",
        "                    dimensions.append(img.size)\n",
        "                    formats.append(img.format)\n",
        "                except Exception as e:\n",
        "                    corrupted.append(str(img_path))\n",
        "    \n",
        "    return dimensions, formats, corrupted\n",
        "\n",
        "# Check train set properties\n",
        "train_dims, train_formats, train_corrupted = check_image_properties(TRAIN_DIR, EMOTIONS, sample_size=200)\n",
        "\n",
        "print(\"Image Properties Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Sample size checked: {len(train_dims)} images\")\n",
        "print(f\"Unique dimensions: {set(train_dims)}\")\n",
        "print(f\"Expected dimension: (48, 48)\")\n",
        "print(f\"Image formats: {set(train_formats)}\")\n",
        "print(f\"Corrupted files: {len(train_corrupted)}\")\n",
        "if train_corrupted:\n",
        "    print(f\"Corrupted files list: {train_corrupted[:5]}...\")  # Show first 5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Grayscale Distribution Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze pixel intensity distribution for each emotion\n",
        "def analyze_pixel_distribution(directory, emotions, num_samples=50):\n",
        "    \"\"\"Analyze pixel intensity distribution for each emotion class.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, emotion in enumerate(emotions):\n",
        "        emotion_dir = directory / emotion\n",
        "        if emotion_dir.exists():\n",
        "            image_files = list(emotion_dir.glob('*.jpg')) + list(emotion_dir.glob('*.png'))\n",
        "            sample_files = np.random.choice(image_files, min(num_samples, len(image_files)), replace=False)\n",
        "            \n",
        "            all_pixels = []\n",
        "            for img_path in sample_files:\n",
        "                try:\n",
        "                    img = np.array(Image.open(img_path))\n",
        "                    all_pixels.extend(img.flatten())\n",
        "                except:\n",
        "                    continue\n",
        "            \n",
        "            if len(all_pixels) > 0:\n",
        "                axes[i].hist(all_pixels, bins=50, alpha=0.7, color=plt.cm.tab10(i))\n",
        "                axes[i].set_title(f'{emotion.capitalize()}\\nMean: {np.mean(all_pixels):.1f}', fontsize=11)\n",
        "                axes[i].set_xlabel('Pixel Intensity', fontsize=9)\n",
        "                axes[i].set_ylabel('Frequency', fontsize=9)\n",
        "                axes[i].grid(alpha=0.3)\n",
        "    \n",
        "    # Remove extra subplot\n",
        "    axes[7].axis('off')\n",
        "    \n",
        "    plt.suptitle('Pixel Intensity Distribution by Emotion Class', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Analyze pixel distribution\n",
        "analyze_pixel_distribution(TRAIN_DIR, EMOTIONS, num_samples=100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dataset Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive summary statistics\n",
        "summary_stats = {\n",
        "    'Metric': [\n",
        "        'Total Images',\n",
        "        'Train Images',\n",
        "        'Test Images',\n",
        "        'Number of Classes',\n",
        "        'Image Size',\n",
        "        'Image Format',\n",
        "        'Most Common Class',\n",
        "        'Least Common Class',\n",
        "        'Class Imbalance Ratio',\n",
        "        'Average Images per Class (Train)',\n",
        "        'Average Images per Class (Test)'\n",
        "    ],\n",
        "    'Value': [\n",
        "        f\"{df_counts['Total'].sum():,}\",\n",
        "        f\"{df_counts['Train_Count'].sum():,}\",\n",
        "        f\"{df_counts['Test_Count'].sum():,}\",\n",
        "        len(EMOTIONS),\n",
        "        '48×48 pixels',\n",
        "        'Grayscale (JPG/PNG)',\n",
        "        df_counts.loc[df_counts['Total'].idxmax(), 'Emotion'],\n",
        "        df_counts.loc[df_counts['Total'].idxmin(), 'Emotion'],\n",
        "        f\"{max_count / min_count:.2f}x\",\n",
        "        f\"{df_counts['Train_Count'].mean():.0f}\",\n",
        "        f\"{df_counts['Test_Count'].mean():.0f}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "df_summary = pd.DataFrame(summary_stats)\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET SUMMARY STATISTICS\")\n",
        "print(\"=\" * 60)\n",
        "print(df_summary.to_string(index=False))\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Known Issues and Considerations\n",
        "\n",
        "### 7.1 Class Imbalance\n",
        "- **Disgust class**: Only 547 images (significantly fewer than other classes)\n",
        "- **Imbalance ratio**: Up to ~16x difference between most and least common classes\n",
        "- **Recommendation**: Consider class weights in loss function or data augmentation for minority classes\n",
        "\n",
        "### 7.2 Image Quality\n",
        "- **Low resolution**: 48×48 pixels (quite small for modern CNNs)\n",
        "- **Grayscale images**: Will need to convert to RGB (3 channels) for EfficientNet\n",
        "- **Upscaling required**: Need to resize to 224×224 for EfficientNet input\n",
        "\n",
        "### 7.3 Label Noise\n",
        "- Emotion recognition is subjective\n",
        "- Some images may be mislabeled\n",
        "- Cultural differences in emotion expression\n",
        "- **Recommendation**: Accept some label noise as inherent to the dataset\n",
        "\n",
        "### 7.4 Dataset Split\n",
        "- Pre-split into train and test sets\n",
        "- Use official test set for final evaluation\n",
        "- Create validation split from training set (10% recommended)\n",
        "\n",
        "### 7.5 Preprocessing Considerations\n",
        "- Convert grayscale to RGB (repeat channel 3 times)\n",
        "- Resize from 48×48 to 224×224\n",
        "- Normalize with ImageNet statistics\n",
        "- Apply data augmentation (horizontal flips, rotations, color jitter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Next Steps\n",
        "\n",
        "1. **Data Preprocessing**: \n",
        "   - Convert grayscale to RGB\n",
        "   - Resize to 224×224\n",
        "   - Normalize with ImageNet stats\n",
        "   - Create train/validation split\n",
        "\n",
        "2. **Model Training**:\n",
        "   - Use EfficientNet-B0 with transfer learning\n",
        "   - Apply class weights for imbalanced classes\n",
        "   - Use data augmentation\n",
        "   - Monitor for overfitting\n",
        "\n",
        "3. **Evaluation**:\n",
        "   - Test on official test set\n",
        "   - Calculate per-class metrics\n",
        "   - Visualize confusion matrix\n",
        "   - Analyze failure cases\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
